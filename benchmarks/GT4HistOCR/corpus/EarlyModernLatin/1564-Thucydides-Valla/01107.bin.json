{
  "mistral-small": {
    "gt": "42",
    "response": "42",
    "wer": 0.0,
    "cer": 0.0,
    "accuracy": 100.0,
    "time": 0.9610631465911865,
    "diffs": [
      [
        0,
        "42"
      ]
    ],
    "matches": 2,
    "deletions": 0,
    "insertions": 0
  },
  "qwen-2.5-vl-72b": {
    "gt": "42",
    "response": "42",
    "wer": 0.0,
    "cer": 0.0,
    "accuracy": 100.0,
    "time": 1.072126865386963,
    "diffs": [
      [
        0,
        "42"
      ]
    ],
    "matches": 2,
    "deletions": 0,
    "insertions": 0
  },
  "spotlight": {
    "gt": "42",
    "response": "42",
    "wer": 0.0,
    "cer": 0.0,
    "accuracy": 100.0,
    "time": 1.221559762954712,
    "diffs": [
      [
        0,
        "42"
      ]
    ],
    "matches": 2,
    "deletions": 0,
    "insertions": 0
  },
  "gpt-4.1": {
    "gt": "42",
    "response": ". 43",
    "wer": 100.0,
    "cer": 100.0,
    "accuracy": 25.0,
    "time": 1.2931671142578125,
    "diffs": [
      [
        -1,
        "42"
      ],
      [
        1,
        ". 43"
      ]
    ],
    "matches": 0,
    "deletions": 2,
    "insertions": 4
  },
  "gpt-4o": {
    "gt": "42",
    "response": ".48",
    "wer": 100.0,
    "cer": 100.0,
    "accuracy": 33.33333333333333,
    "time": 1.357753038406372,
    "diffs": [
      [
        -1,
        "42"
      ],
      [
        1,
        ".48"
      ]
    ],
    "matches": 0,
    "deletions": 2,
    "insertions": 3
  },
  "claude-4-sonnet": {
    "gt": "42",
    "response": "4.",
    "wer": 100.0,
    "cer": 50.0,
    "accuracy": 50.0,
    "time": 2.1256840229034424,
    "diffs": [
      [
        0,
        "4"
      ],
      [
        -1,
        "2"
      ],
      [
        1,
        "."
      ]
    ],
    "matches": 1,
    "deletions": 1,
    "insertions": 1
  },
  "claude-4-opus": {
    "gt": "42",
    "response": "4.",
    "wer": 100.0,
    "cer": 50.0,
    "accuracy": 50.0,
    "time": 2.8333559036254883,
    "diffs": [
      [
        0,
        "4"
      ],
      [
        -1,
        "2"
      ],
      [
        1,
        "."
      ]
    ],
    "matches": 1,
    "deletions": 1,
    "insertions": 1
  },
  "o4-mini": {
    "gt": "42",
    "response": ". 42",
    "wer": 100.0,
    "cer": 100.0,
    "accuracy": 50.0,
    "time": 4.783493995666504,
    "diffs": [
      [
        1,
        ". "
      ],
      [
        0,
        "42"
      ]
    ],
    "matches": 2,
    "deletions": 0,
    "insertions": 2
  },
  "llama-4-maverick-17b": {
    "gt": "42",
    "response": "43",
    "wer": 100.0,
    "cer": 50.0,
    "accuracy": 50.0,
    "time": 5.955574989318848,
    "diffs": [
      [
        0,
        "4"
      ],
      [
        -1,
        "2"
      ],
      [
        1,
        "3"
      ]
    ],
    "matches": 1,
    "deletions": 1,
    "insertions": 1
  },
  "gemini-2.5-pro": {
    "gt": "42",
    "response": ". 48",
    "wer": 100.0,
    "cer": 100.0,
    "accuracy": 25.0,
    "time": 6.478460788726807,
    "diffs": [
      [
        -1,
        "42"
      ],
      [
        1,
        ". 48"
      ]
    ],
    "matches": 0,
    "deletions": 2,
    "insertions": 4
  },
  "gpt-5-mini": {
    "gt": "42",
    "response": ". 42",
    "wer": 100.0,
    "cer": 100.0,
    "accuracy": 50.0,
    "time": 3.9332690238952637,
    "diffs": [
      [
        1,
        ". "
      ],
      [
        0,
        "42"
      ]
    ],
    "matches": 2,
    "deletions": 0,
    "insertions": 2
  },
  "gpt-5": {
    "gt": "42",
    "response": ". 43",
    "wer": 100.0,
    "cer": 100.0,
    "accuracy": 25.0,
    "time": 7.700891017913818,
    "diffs": [
      [
        -1,
        "42"
      ],
      [
        1,
        ". 43"
      ]
    ],
    "matches": 0,
    "deletions": 2,
    "insertions": 4
  },
  "glm-4.5v": {
    "gt": "42",
    "response": "42",
    "wer": 0.0,
    "cer": 0.0,
    "accuracy": 100.0,
    "time": 3.7397451400756836,
    "diffs": [
      [
        0,
        "42"
      ]
    ],
    "matches": 2,
    "deletions": 0,
    "insertions": 0
  },
  "gemini-2.5-flash": {
    "gt": "42",
    "response": "43",
    "wer": 100.0,
    "cer": 50.0,
    "accuracy": 50.0,
    "time": 2.0795302391052246,
    "diffs": [
      [
        0,
        "4"
      ],
      [
        -1,
        "2"
      ],
      [
        1,
        "3"
      ]
    ],
    "matches": 1,
    "deletions": 1,
    "insertions": 1
  },
  "mistral-medium-3.1": {
    "gt": "42",
    "response": "42.",
    "wer": 100.0,
    "cer": 50.0,
    "accuracy": 66.66666666666666,
    "time": 0.8755309581756592,
    "diffs": [
      [
        0,
        "42"
      ],
      [
        1,
        "."
      ]
    ],
    "matches": 2,
    "deletions": 0,
    "insertions": 1
  },
  "ernie-4.5-vl-28b-a3b": {
    "gt": "42",
    "response": ". 4²",
    "wer": 100.0,
    "cer": 100.0,
    "accuracy": 25.0,
    "time": 1.4907269477844238,
    "diffs": [
      [
        -1,
        "42"
      ],
      [
        1,
        ". 4²"
      ]
    ],
    "matches": 0,
    "deletions": 2,
    "insertions": 4
  },
  "grok-4": {
    "gt": "42",
    "response": "4.",
    "wer": 100.0,
    "cer": 50.0,
    "accuracy": 50.0,
    "time": 10.49321699142456,
    "diffs": [
      [
        0,
        "4"
      ],
      [
        -1,
        "2"
      ],
      [
        1,
        "."
      ]
    ],
    "matches": 1,
    "deletions": 1,
    "insertions": 1
  },
  "cogito-v2-109b": {
    "gt": "42",
    "response": "4 3",
    "wer": 100.0,
    "cer": 100.0,
    "accuracy": 33.33333333333333,
    "time": 1.699955940246582,
    "diffs": [
      [
        0,
        "4"
      ],
      [
        -1,
        "2"
      ],
      [
        1,
        " 3"
      ]
    ],
    "matches": 1,
    "deletions": 1,
    "insertions": 2
  },
  "qwen3-vl-235b-instruct": {
    "gt": "42",
    "response": ". 4²",
    "wer": 100.0,
    "cer": 100.0,
    "accuracy": 25.0,
    "time": 1.916365146636963,
    "diffs": [
      [
        -1,
        "42"
      ],
      [
        1,
        ". 4²"
      ]
    ],
    "matches": 0,
    "deletions": 2,
    "insertions": 4
  }
}